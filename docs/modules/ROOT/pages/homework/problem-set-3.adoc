= Problem Set 3: A Posteriori Error Bounds, Greedy Sampling Procedure 
:page-jupyter: true
:page-plotly: true
:stem: latexmath
Christophe Prud’homme


We consider again the problem of designing a thermal fin of Problem Set 1 and 2. Given the reduced basis approximation implemented in PS2, we turn to implementing the associated a posteriori error estimation procedures developed in the lecture. The second half of this problem set is devoted to implementing the greedy sampling procedure. We will consider the following two cases:

Case I (latexmath:[P=1]):: We keep the Biot number fixed at latexmath:[Bi = 0.1] and assume that the conductivities of all fins are equivalent, i.e., latexmath:[k = k_1 = k_2 = k_3 = k_4], but are allowed to vary between 0.1 and 10 — we thus have latexmath:[\mu \in D = [0.1, 10].] For this latexmath:[P = 1] case we define the latexmath:[X]-inner product latexmath:[(\cdot, \cdot)_X = a(\cdot, \cdot; \bar{\mu}),] where latexmath:[\bar{\mu} = 1.]

Case II (latexmath:[P = 2]):: We again assume that the conductivities of all fins are equivalent, i.e., latexmath:[k = k_1 = k_2 = k_3 = k_4] , but are allowed to vary between 0.1 and 10; furthermore, we assume that the Biot number satisfies latexmath:[0.01 \leq Bi \leq 1.] We thus have latexmath:[\mu = (k, Bi) = [0.1, 10\] \times [0.01, 1\].] For this latexmath:[P = 2] case we define the latexmath:[X]-inner product latexmath:[(\cdot, \cdot)_X = a(\cdot, \cdot; \bar{\mu}),] where latexmath:[\bar{\mu} = (1, 0.1)] (the two inner products are in fact the same since latexmath:[Bi = 0.1] here).

We also define the parameter grids latexmath:[G^{\mathrm{lin}}_{[ \mu_{min} , \mu_{max} ;10\]}] and latexmath:[G^{\mathrm{ln}}_{[ \mu_{min} , \mu_{max} ;10\]}]. The former grid is equi-spaced in latexmath:[\mu], the latter grid is equi-spaced in latexmath:[ln(\mu)] — often advantageous within the reduced basis context. More generally, the "`log`" spacing represents equal relative increments, and thus represents better coverage for parameters that vary over a large range. For the latexmath:[P = 2] case we can then define tensor grids over latexmath:[\mathcal{D}], latexmath:[\Xi^{\mathrm{log}}_M  \subset D  \subset \mathbb{R}^2] , as

[latexmath]
++++
\Xi^{log}_M = G^{log}_{[ \mu_{min} , \mu_{max} ;M ]} \times G^{log}_{[ \mu_{min} , \mu_{max} ;M ]} ;
++++
note latexmath:[\Xi^{log}_M] contains latexmath:[M^2] points; a similar definition applies to latexmath:[\Xi^{lin}_M]. We also define a particular test grid (biased neither towards "`log`" nor "`lin`")

[latexmath]
++++
\Xi^{test}_M =  \Xi^{lin}_M \cup \Xi^{log}_M ;
++++
note latexmath:[\Xi^{test}_M] contains latexmath:[2M^2] points.

[[sec:1]]
== Part 1 - Coercivity Lower Bound and X-Norm Error Bound

We first consider the construction of the lower bound for the coercivity constant.

=== Q1.

Since our problem is parametrically coercive, the simple latexmath:[\min \theta]-approach suffices for the construction of the coercivity lower bound, latexmath:[\alpha_{LB} (\mu)]. However, we have to slightly adapt the lower bound to Case I and II.

[loweralpha]
. Derive an explicit expression for latexmath:[\alpha_{LB} (\mu)] for Case I and Case II. 

. What is the largest effectivity for the energy norm error bound and the output error bound we should anticipate for Case I and Case II?

=== Q2.

Prove the bounding properties of the latexmath:[X]-norm error bound, i.e., the effectivity latexmath:[\eta_N(\mu)] satisfies

[latexmath]
++++
1 \leq \eta_N(\mu) \leq \frac{\gamma_{UB} (\mu)}{\alpha_{LB} (\mu)}, \quad \forall \mu \in \mathcal{D}.
++++


== Part 2 - A Posteriori Error Estimation

Given the coercivity lower bound, we can now turn to implementing the a posteriori error bounds. Note that, in principle, there is an online-inefficient and an online-efficient way to evaluate these error bounds. We first consider the latter: From the lecture we know that the energy norm a posteriori error bound is given by

[latexmath#eq:2.1]
++++
\Delta^{en}_N(\mu)= \frac{\|\hat{e}(\mu\|}{\sqrt{\alpha_{LB}(\mu)}}
++++
where latexmath:[\hat{e}(\mu) \in X] satisfies

[latexmath#eq:2.2]
++++
(\hat{e}(\mu), v)_X = r(v; \mu), \quad \forall v \in X,
++++
and the residual is given by

[latexmath#eq:2.3]
++++
r(v; \mu) = f (v; \mu) - a(u_N (\mu), v; \mu),\quad \forall v \in X.
++++

For any new latexmath:[\mu] and associated reduced basis solution, latexmath:[u_N (\mu),] we can now directly calculate latexmath:[\hat{e}(\mu)] from <<eq:2.2,2.2>> and <<eq:2.3,2.3>>, evaluate the norm latexmath:[\|\hat{e}(\mu)||_X] and — given latexmath:[\alpha_{LB} (\mu)] — obtain latexmath:[\Delta^{en}_N (\mu)] from <<eq:2.1,2.1>>. Although this approach is online-inefficient because the computational cost depends on latexmath:[O(\mathcal{N})], it is nevertheless useful as a means to test your offline-online computational decomposition. We will consider Case I and Case II in the sequel. Note that you should only require one  code to handle both cases, i.e., Case I is a specialization of Case II by keeping one of the parameters fixed. Also, when using  you should try to replace loops by matrix-vector products as much as possible (e.g. try to write the nested loop over latexmath:[N] when summing up the contributions of the latexmath:[\|\hat{e}(\mu)\|_X] norm as a vector-matrix-vector product — the nested loop over latexmath:[Q_a] is less critical).

=== Q3.

We first consider Case I. To answer this question you should use the sample set latexmath:[S_N] provided for PS2 (`+RB_sample.sample1+`), orthonormalize the basis functions, and use the medium grid.

[loweralpha]
. Implement an offline/online version of the a posteriori error bound calculation following the computational decomposition shown in the lecture. Show that the direct calculation and the offline-online decomposition deliver the same results for the error bound, latexmath:[\Delta^{en}_N (\mu)], for all latexmath:[N (1 \leq N \leq 8)] and (say) latexmath:[5] parameter values randomly distributed in latexmath:[\mathcal{D}.]

. Calculate latexmath:[\eta^{en}_{\min,N},\eta^{en}_{\max,N}] and latexmath:[\eta^{en}_{ave,N}] the minimum, maximum, and average effectivity latexmath:[\eta^{en}_N(\mu)] over latexmath:[\Xi test = G^{lin}[ \mu_{min} , \mu_{max} ;50] \cup G^{ln}[ \mu_{min} , \mu_{max} ;50\]], respectively (note that latexmath:[\Xi^{test}] is of size 100 since latexmath:[P = 1]).

Present the results in a table for all latexmath:[N] . Is the minimum effectivity greater than unity? How does the maximum effectivity compare with your theoretical upper bound for the effectivity? (Note you should exclude from the min/max/mean-operation all points in latexmath:[\Xi^{test}] for which latexmath:[\|u(\mu) - u_N (\mu)\|_X] is less than (say) latexmath:[10e-11] .)

. Evalulate the effectivity for latexmath:[\mu = 1] for latexmath:[N = 1,\ldots, 8]. What do you observe? Justify your answer. (d ) Evaluate the exact error, latexmath:[\|u(\mu) - u_N (\mu)\|_X] , and error bound for latexmath:[\mu = 0.1]. What do you observe? Justify your answer.

=== Q4.

We consider Case II. To answer this question you should use the sample set latexmath:[S_N] provided for the PS2 (`+RB_sample.sample3+`), orthonormalize the basis functions, and use the medium grid.

. Implement an offline/online version of the a posteriori error bound calculation following the computational decomposition shown in the lecture. Show that the direct calculation and the offline-online decomposition deliver the same results for the error bound, latexmath:[\Delta^{en}_N (\mu)], for all latexmath:[N] latexmath:[(1 \leq N \leq 46)] and (say) 5 parameter values randomly distributed in latexmath:[\mathcal{D}].

. Calculate latexmath:[\eta^{en}_{\min,N},\eta^{en}_{\max,N}] and latexmath:[\eta^{en}_{ave,N}] the minimum, maximum, and average effectivity latexmath:[\eta^{en}_N(\mu)] over latexmath:[\Xi^{test}_M = \Xi^{lin}_{M=10}\cup \Xi^{log}_{M=10}], respectively. Present the results in a table for latexmath:[N = 5, 10, 20,30, 40.] Is the minimum effectivity greater than unity? How does the maximum effectivity compare with your theoretical upper bound for the effectivity? (Note you should again exclude from the min/max/mean-operation all points in latexmath:[\Xi^{test}_M] for which latexmath:[\|u(\mu) - u_N (\mu)\|_X] is less than (say) latexmath:[10e-11].)

== Part 3 - Reduced Basis Output Bound

Given the a posteriori error bound from Part 2 we can now directly evaluate the output error bound.

=== Q5.

We consider Case II. To answer this question you should use the sample set SN provided for PS2 (`+RB_sample.sample3+`), orthnormalize the basis functions, and use the medium grid.

[loweralpha]
. Extend your  code to also calculate the output error bound.

. Calculate latexmath:[\eta^{s}_{\min,N},\eta^{s}_{\max,N}] and latexmath:[\eta^{s}_{ave,N}] the minimum, maximum, and average effectivity latexmath:[\eta^{s}_N(\mu)] over latexmath:[\Xi^{test}_M =  \Xi^{lin}_M =10 \cup  \Xi^{log}_M =10], respectively. Present the results in a table for latexmath:[N = 5, 10, 20,30, 40.] How does the maximum effectivity compare with your theoretical upper bound for the effectivity? (Note you should exclude from the min/max/mean-operation all points in latexmath:[\Xi^{test}] for which latexmath:[|s(\mu) - s_N (\mu)|] is less than (say) latexmath:[10e-11] .)

. What value of latexmath:[N] do you require to achieve a relative accuracy in the output bound of approximately 1%? What is the true error for this value of latexmath:[N] ?

. How does the online computational cost to calculate latexmath:[\Delta^s_N (\mu)] compare to the online computational cost to calculate latexmath:[s_N (\mu)] as a function of latexmath:[N] (take the average over the test sample latexmath:[\Xi^{test}_M] )?

. How does the computational cost to calculate the truth output latexmath:[s(\mu)] compare to the online computational cost to calculate latexmath:[s_N (\mu)] and latexmath:[\Delta^s_N (\mu)] as a function of latexmath:[N] (take the average over the test sample latexmath:[\Xi^{test}_M] )?


== Part 4 - Greedy Sampling Procedure

Given your (now tested and - hopefully - functioning) offline-online computational decomposition for the reduced basis approximation and associated a posteriori error estimation, we turn to the Greedy Sampling Procedure. In PS2 you where given the sample sets latexmath:[S_N] — now you can construct these yourself.

For this problem set, you should use the algorithm with latexmath:[\omega(\mu) =
|||u_N (\mu)|||_\mu] (note that we can calculate latexmath:[|||u_N (\mu)|||_\mu] online-efficient in latexmath:[O(N^2)] operations — as opposed to latexmath:[|||u(\mu)|||_\mu] which would require latexmath:[O(\mathcal{N})] operations). We set the desired error tolerance to latexmath:[\varepsilon_{tol,\min} =
10e-6] and choose latexmath:[S_1 =  \mu_{min}] and latexmath:[X_1 = \mathrm{span}\{u( \mu_{min} )\}.]

Note that there are many steps implicit in the greedy loop. In particular, after the update latexmath:[S_N = S_{N-1} \cup \mu^{*}_N] , we must calculate latexmath:[u(\mu^{*}_N )] to construct (using Gram-Schmidt) the new contribution to our orthonormal basis set, latexmath:[\zeta_N] , to "`form`" latexmath:[X_N] , and finally calculate all the necessary online quantities for both our reduced basis approximation and associated a posteriori error estimation. We note here a practical point for our hierarchical space: as we proceed from latexmath:[N] to latexmath:[N + 1], we should only compute the necessary incremental quantities — the incremental contributions to the various online inner-product arrays required for the reduced basis approximation and a posteriori error estimators.

== Q6.

We consider Case I. Apply the greedy algorithm with latexmath:[\Xi^{train} = G^{ln}_{[ \mu_{min} , \mu_{max} ;100\]} , S_1 = \mu_{min} = 0.1] and latexmath:[\varepsilon_{tol,min} = 1e-6].

[loweralpha]
. What is the value of stem:[N_{max}] to achieve the desired accuracy? In a sequence of stem:[N_{max}] figures (or subplots), plot the relative exact error latexmath:[\|u(\mu) - u_N (\mu)\|_X /|||u_N (\mu)|||_\mu] and the relative energy error bound, latexmath:[\Delta^{en}_N (\mu)/|||u_N (\mu)|||_\mu] , over latexmath:[\mu \in  \Xi^{train}] . In each plot, mark the parameter value which is picked by the greedy procedure in this step.

. Plot latexmath:[\Delta_N^{max}] as a function of latexmath:[N] .

. Generate a non-hierarchical reduced basis approximation for latexmath:[S^{lin}_N=G^{lin}_{[ \mu_{min} , \mu_{max} ;N \]}] and latexmath:[S^{ln}_N =
G^{ln}_{[ \mu_{min} , \mu_{max} ;N \]}] with latexmath:[2 \leq N \leq N_{max}] . We would like to compare the convergence of the reduced basis approximation generated using the greedy algorithm and the reduced basis approximations from the linear and logarithmic sample. Plot the convergence of the maximum relative error in the energy norm latexmath:[max_{\mu \in \Xi^test} |||u(\mu) - u_N (\mu)|||_\mu /|||u(\mu)|||_\mu] as a function of latexmath:[N] for all ln three cases in one plot. Here, latexmath:[\Xi^{test} = G^{lin}_{[ \mu_{min} , \mu_{max} ;50\]} \cup G^{ln}[ \mu_{min} , \mu_{max} ;50\]] is a test sample of size latexmath:[n_{test} = 100.]

== Q7.

We consider Case II. 

Apply the greedy algorithm with latexmath:[\Xi^{train} =  \Xi^{log}_M] (the log tensor product grid with latexmath:[M = 25]), latexmath:[S_1 = \mu_{min} = (0.1, 0.01)], and latexmath:[\varepsilon_{tol,min} = 10e-6] .

[loweralpha]
. What is the value of stem:[N_{max}] to achieve the desired accuracy? 
. Plot latexmath:[\Delta_N^{max}] as a function of latexmath:[N].

. Plot your greedy samples latexmath:[S_N] ; present your results as dots in the latexmath:[(ln \mu_1 , ln \mu_2 )] plane. Can you attribute the observed distribution of parameter points to any mathematical or physical causes?

. For the reduced basis approximation you just generated, plot the convergence of the maximum relative error in the energy norm latexmath:[\max_{\mu \in \Xi^{test}} |||u(\mu) - u_N (\mu)|||_\mu /|||u(\mu)|||_\mu] and the maximum relative output error latexmath:[\max_{\mu\in \Xi^{test}} |{T_{root}} (\mu) - {T_{root}}_N (\mu)|/{T_{root}} (\mu)] as a function of latexmath:[N] . Use latexmath:[\Xi^{test} = \Xi^{test}_M] with latexmath:[M = 10] (the combined linear and logarithmic tensor product grid).
