{"cells":[{"cell_type":"markdown","source":["# Problem Set 3: A Posteriori Error Bounds, Greedy Sampling Procedure\n\n","We consider again the problem of designing a thermal fin of Problem Set 1 and 2. Given the reduced basis approximation implemented in PS2, we turn to implementing the associated a posteriori error estimation procedures developed in the lecture. The second half of this problem set is devoted to implementing the greedy sampling procedure. We will consider the following two cases:\n"],"metadata":{}},{"cell_type":"markdown","source":["\n","- Case I ($P=1$): We keep the Biot number fixed at $Bi = 0.1$ and assume that the conductivities of all fins are equivalent, i.e., $k = k_1 = k_2 = k_3 = k_4$, but are allowed to vary between 0.1 and 10 — we thus have $\\mu \\in D = [0.1, 10$.] For this $P = 1$ case we define the $X$-inner product $(\\cdot, \\cdot)_X = a(\\cdot, \\cdot; \\bar{\\mu}),$ where $\\bar{\\mu} = 1.$\n"],"metadata":{"node_name":"ulist"}},{"cell_type":"markdown","source":["- Case II ($P = 2$): We again assume that the conductivities of all fins are equivalent, i.e., $k = k_1 = k_2 = k_3 = k_4$ , but are allowed to vary between 0.1 and 10; furthermore, we assume that the Biot number satisfies $0.01 \\leq Bi \\leq 1.$ We thus have $\\mu = (k, Bi) = [0.1, 10] \\times [0.01, 1].$ For this $P = 2$ case we define the $X$-inner product $(\\cdot, \\cdot)_X = a(\\cdot, \\cdot; \\bar{\\mu}),$ where $\\bar{\\mu} = (1, 0.1)$ (the two inner products are in fact the same since $Bi = 0.1$ here).\n","\n"],"metadata":{"node_name":"ulist"}},{"cell_type":"markdown","source":["We also define the parameter grids $G^{\\mathrm{lin}}_{[ \\mu_{min} , \\mu_{max} ;10]}$ and $G^{\\mathrm{ln}}_{[ \\mu_{min} , \\mu_{max} ;10]}$. The former grid is equi-spaced in $\\mu$, the latter grid is equi-spaced in $ln(\\mu)$ — often advantageous within the reduced basis context. More generally, the log spacing represents equal relative increments, and thus represents better coverage for parameters that vary over a large range. For the $P = 2$ case we can then define tensor grids over $\\mathcal{D}$, $\\Xi^{\\mathrm{log}}_M  \\subset D  \\subset \\mathbb{R}^2$ , as\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\Xi^{log}_M = G^{log}_{[ \\mu_{min} , \\mu_{max} ;M ]} \\times G^{log}_{[ \\mu_{min} , \\mu_{max} ;M ]} ;\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["note $\\Xi^{log}_M$ contains $M^2$ points; a similar definition applies to $\\Xi^{lin}_M$. We also define a particular test grid (biased neither towards log nor lin)\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\Xi^{test}_M =  \\Xi^{lin}_M \\cup \\Xi^{log}_M ;\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["note $\\Xi^{test}_M$ contains $2M^2$ points.\n\n","## Part 1 - Coercivity Lower Bound and X-Norm Error Bound\n\n","We first consider the construction of the lower bound for the coercivity constant.\n","## Q1.\n\n","Since our problem is parametrically coercive, the simple $\\min \\theta$-approach suffices for the construction of the coercivity lower bound, $\\alpha_{LB} (\\mu)$. However, we have to slightly adapt the lower bound to Case I and II.\n","\f(a) Derive an explicit expression for $\\alpha_{LB} (\\mu)$ for Case I and Case II. (b) What is the largest effectivity for the energy norm error bound and the output error bound we should anticipate for Case I and Case II?\n","## Q2.\n\n","Prove the bounding properties of the $X$-norm error bound, i.e., the effectivity $\\eta_N(\\mu)$ satisfies\n","\n$$\n1 \\leq \\eta_N(\\mu) \\leq \\frac{\\gamma_{UB} (\\mu)}{\\alpha_{LB} (\\mu)}, \\quad \\forall \\mu \\in \\mathcal{D}.\n$$\n\n","## Part 2 - A Posteriori Error Estimation\n\n","Given the coercivity lower bound, we can now turn to implementing the a posteriori error bounds. Note that, in principle, there is an online-inefficient and an online-efficient way to evaluate these error bounds. We first consider the latter: From the lecture we know that the energy norm a posteriori error bound is given by\n","\n$$\n\\Delta^{en}_N(\\mu)= \\frac{\\|\\hat{e}(\\mu\\|}{\\sqrt{\\alpha_{LB}(\\mu)}}\n$$\n","where $\\hat{e}(\\mu) \\in X$ satisfies\n","\n$$\n(\\hat{e}(\\mu), v)_X = r(v; \\mu), \\quad \\forall v \\in X,\n$$\n","and the residual is given by\n","\n$$\nr(v; \\mu) = f (v; \\mu) - a(u_N (\\mu), v; \\mu),\\quad \\forall v \\in X.\n$$\n","For any new $\\mu$ and associated reduced basis solution, $u_N (\\mu),$ we can now directly calculate $\\hat{e}(\\mu)$ from [2.2](#eq:2.2) and [2.3](#eq:2.3), evaluate the norm $\\|\\hat{e}(\\mu)||_X$ and — given $\\alpha_{LB} (\\mu)$ — obtain $\\Delta^{en}_N (\\mu)$ from [2.1](#eq:2.1). Although this approach is online-inefficient because the computational cost depends on $O(\\mathcal{N})$, it is nevertheless useful as a means to test your offline-online computational decomposition. We will consider Case I and Case II in the sequel. Note that you should only require one  code to handle both cases, i.e., Case I is a specialization of Case II by keeping one of the parameters fixed. Also, when using  you should try to replace loops by matrix-vector products as much as possible (e.g. try to write the nested loop over $N$ when summing up the contributions of the $\\|\\hat{e}(\\mu)\\|_X$ norm as a vector-matrix-vector product — the nested loop over $Q_a$ is less critical).\n","## Q3.\n\n","We first consider Case I. To answer this question you should use the sample set $S_N$ provided for PS2 (`RB_sample.sample1`), orthonormalize the basis functions, and use the medium grid.\n","\n","1. Implement an offline/online version of the a posteriori error bound calculation following the computational decomposition shown in the lecture. Show that the direct calculation and the offline-online decomposition deliver the same results for the error bound, $\\Delta^{en}_N (\\mu)$, for all $N (1 \\leq N \\leq 8)$ and (say) $5$ parameter values randomly distributed in $\\mathcal{D}.$\n","1. Calculate $\\eta^{en}_{\\min,N},\\eta^{en}_{\\max,N}$ and $\\eta^{en}_{ave,N}$ the minimum, maximum, and average effectivity $\\eta^{en}_N(\\mu)$ over $\\Xi test = G^{lin}[ \\mu_{min} , \\mu_{max} ;50$ \\cup G^{ln}[ \\mu_{min} , \\mu_{max} ;50]], respectively (note that $\\Xi^{test}$ is of size 100 since $P = 1$).\n","\n","Present the results in a table for all $N$ . Is the minimum effectivity greater than unity? How does the maximum effectivity compare with your theoretical upper bound for the effectivity? (Note you should exclude from the min/max/mean-operation all points in $\\Xi^{test}$ for which $\\|u(\\mu) - u_N (\\mu)\\|_X$ is less than (say) $10e-11$ .)\n","\n","1. Evalulate the effectivity for $\\mu = 1$ for $N = 1,\\ldots, 8$. What do you observe? Justify your answer. (d ) Evaluate the exact error, $\\|u(\\mu) - u_N (\\mu)\\|_X$ , and error bound for $\\mu = 0.1$. What do you observe? Justify your answer.\n","## Q4.\n\n","We consider Case II. To answer this question you should use the sample set $S_N$ provided for the PS2 (`RB_sample.sample3`), orthonormalize the basis functions, and use the medium grid.\n","\n","1. Implement an offline/online version of the a posteriori error bound calculation following the computational decomposition shown in the lecture. Show that the direct calculation and the offline-online decomposition deliver the same results for the error bound, $\\Delta^{en}_N (\\mu)$, for all $N$ $(1 \\leq N \\leq 46)$ and (say) 5 parameter values randomly distributed in $\\mathcal{D}$.\n","1. Calculate $\\eta^{en}_{\\min,N},\\eta^{en}_{\\max,N}$ and $\\eta^{en}_{ave,N}$ the minimum, maximum, and average effectivity $\\eta^{en}_N(\\mu)$ over $\\Xi^{test}_M = \\Xi^{lin}_{M=10}\\cup \\Xi^{log}_{M=10}$, respectively. Present the results in a table for $N = 5, 10, 20,30, 40.$ Is the minimum effectivity greater than unity? How does the maximum effectivity compare with your theoretical upper bound for the effectivity? (Note you should again exclude from the min/max/mean-operation all points in $\\Xi^{test}_M$ for which $\\|u(\\mu) - u_N (\\mu)\\|_X$ is less than (say) $10e-11$.)\n","\n\n","## Part 3 - Reduced Basis Output Bound\n\n","Given the a posteriori error bound from Part 2 we can now directly evaluate the output error bound.\n","## Q5.\n\n","We consider Case II. To answer this question you should use the sample set SN provided for PS2 (`RB_sample.sample3`), orthnormalize the basis functions, and use the medium grid.\n","\n","1. Extend your  code to also calculate the output error bound.\n","1. Calculate $\\eta^{s}_{\\min,N},\\eta^{s}_{\\max,N}$ and $\\eta^{s}_{ave,N}$ the minimum, maximum, and average effectivity $\\eta^{s}_N(\\mu)$ over $\\Xi^{test}_M =  \\Xi^{lin}_M =10 \\cup  \\Xi^{log}_M =10$, respectively. Present the results in a table for $N = 5, 10, 20,30, 40.$ How does the maximum effectivity compare with your theoretical upper bound for the effectivity? (Note you should exclude from the min/max/mean-operation all points in $\\Xi^{test}$ for which $|s(\\mu) - s_N (\\mu)|$ is less than (say) $10e-11$ .)\n","1. What value of $N$ do you require to achieve a relative accuracy in the output bound of approximately 1%? What is the true error for this value of $N$ ?\n","1. How does the online computational cost to calculate $\\Delta^s_N (\\mu)$ compare to the online computational cost to calculate $s_N (\\mu)$ as a function of $N$ (take the average over the test sample $\\Xi^{test}_M$ )?\n","1. How does the computational cost to calculate the truth output $s(\\mu)$ compare to the online computational cost to calculate $s_N (\\mu)$ and $\\Delta^s_N (\\mu)$ as a function of $N$ (take the average over the test sample $\\Xi^{test}_M$ )?\n","\n\n","## Part 4 - Greedy Sampling Procedure\n\n","Given your (now tested and - hopefully - functioning) offline-online computational decomposition for the reduced basis approximation and associated a posteriori error estimation, we turn to the \fGreedy Sampling Procedure. In PS2 you where given the sample sets $S_N$ — now you can construct these yourself.\n","For this problem set, you should use the algorithm with latexmath:[\\omega(\\mu) =\n","|||u_N (\\mu)|||*\\mu] (note that we can calculate $|||u_N (\\mu)|||_\\mu$ online-efficient in $O(N^2)$ operations — as opposed to $|||u(\\mu)|||_\\mu$ which would require $O(\\mathcal{N})$ operations). We set the desired error tolerance to latexmath:[\\varepsilon*{tol,\\min} =\n","10e-6] and choose $S_1 =  \\mu_{min}$ and $X_1 = \\mathrm{span}\\{u( \\mu_{min} )\\}.$\n","Note that there are many steps implicit in the greedy loop. In particular, after the update $S_N = S_{N-1} \\cup \\mu^{*}_N$ , we must calculate $u(\\mu^{*}_N )$ to construct (using Gram-Schmidt) the new contribution to our orthonormal basis set, $\\zeta_N$ , to form $X_N$ , and finally calculate all the necessary online quantities for both our reduced basis approximation and associated a posteriori error estimation. We note here a practical point for our hierarchical space: as we proceed from $N$ to $N + 1$, we should only compute the necessary incremental quantities — the incremental contributions to the various online inner-product arrays required for the reduced basis approximation and a posteriori error estimators.\n\n","## Q6.\n\n","We consider Case I. Apply the greedy algorithm with $\\Xi^{train} = G^{ln}_{[ \\mu_{min} , \\mu_{max} ;100]} , S_1 = \\mu_{min} = 0.1$ and $\\varepsilon_{tol,min} = 1e-6$.\n","\n","1. What is the value of Nmax to achieve the desired accuracy? In a sequence of Nmax figures (or subplots), plot the relative exact error $\\|u(\\mu) - u_N (\\mu)\\|_X /|||u_N (\\mu)|||_\\mu$ and the relative energy error bound, $\\Delta^{en}_N (\\mu)/|||u_N (\\mu)|||_\\mu$ , over $\\mu \\in  \\Xi^{train}$ . In each plot, mark the parameter value which is picked by the greedy procedure in this step.\n","1. Plot $\\Delta_N^{max}$ as a function of $N$ .\n","1. Generate a non-hierarchical reduced basis approximation for $S^{lin}_N=G^{lin}_{[ \\mu_{min} , \\mu_{max} ;N ]}$ and $S^{ln}_N =\nG^{ln}_{[ \\mu_{min} , \\mu_{max} ;N ]}$ with $2 \\leq N \\leq N_{max}$ . We would like to compare the convergence of the reduced basis approximation generated using the greedy algorithm and the reduced basis approximations from the linear and logarithmic sample. Plot the convergence of the maximum relative error in the energy norm $max_{\\mu \\in \\Xi^test} |||u(\\mu) - u_N (\\mu)|||_\\mu /|||u(\\mu)|||_\\mu$ as a function of $N$ for all ln three cases in one plot. Here, $\\Xi^{test} = G^{lin}_{[ \\mu_{min} , \\mu_{max} ;50]} \\cup G^{ln}[ \\mu_{min} , \\mu_{max} ;50]$ is a test sample of size $n_{test} = 100.$\n","\n\n","## Q7.\n\n","We consider Case II.\n","Apply the greedy algorithm with $\\Xi^{train} =  \\Xi^{log}_M$ (the log tensor product grid with $M = 25$), $S_1 = \\mu_{min} = (0.1, 0.01)$, and $\\varepsilon_{tol,min} = 10e-6$ .\n","\n","1. What is the value of Nmax to achieve the desired accuracy? (b) Plot $\\Delta_N^{max}$ as a function of $N$.\n","1. Plot your greedy samples $S_N$ ; present your results as dots in the $(ln \\mu_1 , ln \\mu_2 )$ plane. Can you attribute the observed distribution of parameter points to any mathematical or physical causes?\n","1. For the reduced basis approximation you just generated, plot the convergence of the maximum relative error in the energy norm $\\max_{\\mu \\in \\Xi^{test}} |||u(\\mu) - u_N (\\mu)|||_\\mu /|||u(\\mu)|||_\\mu$ and the maximum relative output error $\\max_{\\mu\\in \\Xi^{test}} |Troot (\\mu) - Troot_N (\\mu)|/Troot (\\mu)$ as a function of $N$ . Use $\\Xi^{test} = \\Xi^{test}_M$ with $M = 10$ (the combined linear and logarithmic tensor product grid).\n","\n"],"metadata":{}}],"metadata":{"language_info":{"name":"python","version":"3.9.1"},"kernelspec":{"name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":4}